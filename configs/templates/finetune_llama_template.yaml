# Example configuration for src/finetune_llama_kfold.py
# Copy this file into configs/private/ and adjust the paths to match your
# environment.  The template mirrors the top-6-layer setup used in the paper.

gpu: 0  # index of the GPU you want to use

train_data_path: './data/train_set_kaggle.csv'
k_folds: 5
wandb_on: false
use_subset: false
use_prompt: true
save_best_model: true

model:
  path: '/path/to/Meta-Llama-3.2-1B'   # download separately
  version: '3.2'
  num_parameters: '1B'

checkpoint_dir: './checkpoints/'
prompt_path: './prompts/prompt_template_1.txt'

training_strategy: 'top_n_layers'
layers_to_unfreeze: 6

epochs: 100
batch_size: 1
max_len: 1214

learning_rate:
  last_layer_only: 5e-5
  all_layers: 2e-5
  top_n_layers: 5e-5
  prompt_tuning: 2e-3

patience: 15

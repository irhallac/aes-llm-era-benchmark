{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/datastore/ibrahimh/conda_envs/gpu_39_env1/bin/python\n",
      "/cluster/datastore/ibrahimh/projects/aes_paper_dev\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "print(sys.executable)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Loading model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/datastore/ibrahimh/conda_envs/gpu_39_env1/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Extracting MiniLM embeddings...\n",
      "\n",
      "‚úÖ Saved MiniLM embeddings to dataset/embeddings_transformer/MiniLM_embeddings.npz\n",
      "Total extracted embeddings: 3911\n",
      "Total labels: 3911\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ‚úÖ Load dataset\n",
    "file_path = \"dataset/train.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# ‚úÖ Ensure expected columns exist\n",
    "expected_columns = [\"text_id\", \"full_text\", \"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]\n",
    "if not all(col in data.columns for col in expected_columns):\n",
    "    raise ValueError(f\"Dataset is missing required columns. Expected: {expected_columns}\")\n",
    "\n",
    "# ‚úÖ Drop rows with missing text\n",
    "data = data.dropna(subset=[\"full_text\"]).reset_index(drop=True)\n",
    "\n",
    "# ‚úÖ Extract text data\n",
    "texts = data[\"full_text\"].tolist()\n",
    "labels = data[[\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]].values\n",
    "\n",
    "# ‚úÖ Load MiniLM model\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(f\"\\nüîç Loading model: {MODEL_NAME}\")\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# ‚úÖ Extract embeddings\n",
    "print(\"\\nüöÄ Extracting MiniLM embeddings...\")\n",
    "embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "# ‚úÖ Save embeddings and labels properly\n",
    "output_path = \"dataset/embeddings_transformer/MiniLM_embeddings.npz\"\n",
    "np.savez(output_path, embeddings=embeddings, labels=labels)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved MiniLM embeddings to {output_path}\")\n",
    "print(f\"Total extracted embeddings: {embeddings.shape[0]}\")\n",
    "print(f\"Total labels: {labels.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the file: ['embeddings', 'labels']\n",
      "Embeddings shape: (3911, 384)\n",
      "Labels shape: (3911, 6)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"dataset/embeddings_transformer/MiniLM_embeddings.npz\")\n",
    "print(\"Keys in the file:\", list(data.keys()))  # Should print: ['embeddings', 'labels']\n",
    "print(\"Embeddings shape:\", data[\"embeddings\"].shape)\n",
    "print(\"Labels shape:\", data[\"labels\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç **Loading MiniLM embeddings**\n",
      "üöÄ Training XGBoost on MiniLM embeddings...\n",
      "XGBoost - MAE: 0.3972, QWK: 0.3224\n",
      "\n",
      "‚úÖ Benchmarking complete! Results saved in benchmark_results_transformers_xgb.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, cohen_kappa_score\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths to MiniLM embeddings\n",
    "EMBEDDINGS_FILE = \"dataset/embeddings_transformer/MiniLM_embeddings.npz\"\n",
    "\n",
    "# Load MiniLM embeddings\n",
    "print(\"\\nüîç **Loading MiniLM embeddings**\")\n",
    "data = np.load(EMBEDDINGS_FILE)\n",
    "embeddings, labels = data[\"embeddings\"], data[\"labels\"]\n",
    "\n",
    "# Define Train/Test Split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define valid grade levels\n",
    "VALID_GRADES = torch.tensor([1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])\n",
    "\n",
    "def round_to_valid_grades(predictions):\n",
    "    \"\"\"Round predictions to the nearest valid grade.\"\"\"\n",
    "    return np.array([\n",
    "        float(VALID_GRADES[torch.argmin(torch.abs(VALID_GRADES - pred))])\n",
    "        for pred in predictions\n",
    "    ])\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    \"\"\"Calculate QWK.\"\"\"\n",
    "    grade_to_bin = {float(grade): idx for idx, grade in enumerate(VALID_GRADES)}\n",
    "    y_true_bins = [grade_to_bin[float(val)] for val in y_true]\n",
    "    y_pred_bins = [grade_to_bin[float(val)] for val in y_pred]\n",
    "\n",
    "    if len(set(y_true_bins)) == 1 or len(set(y_pred_bins)) == 1:\n",
    "        return 0.0  # Prevents QWK crash when only one class exists\n",
    "\n",
    "    return cohen_kappa_score(y_true_bins, y_pred_bins, weights=\"quadratic\")\n",
    "\n",
    "# üöÄ Train XGBoost (much faster than Random Forest)\n",
    "print(\"üöÄ Training XGBoost on MiniLM embeddings...\")\n",
    "#xgb_model = XGBRegressor(n_estimators=200, max_depth=10, learning_rate=0.1, n_jobs=-1, random_state=42)\n",
    "xgb_model = XGBRegressor(n_estimators=500, max_depth=15, learning_rate=0.05, n_jobs=-1, random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train, y_train.mean(axis=1))\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = xgb_model.predict(X_test)\n",
    "predictions_rounded = round_to_valid_grades(predictions)\n",
    "y_test_rounded = round_to_valid_grades(y_test.mean(axis=1))\n",
    "\n",
    "# Compute MAE and QWK\n",
    "mae = mean_absolute_error(y_test_rounded, predictions_rounded)\n",
    "qwk = quadratic_weighted_kappa(y_test_rounded, predictions_rounded)\n",
    "\n",
    "# Save results\n",
    "results_file = \"benchmark_results_transformers_xgb.txt\"\n",
    "with open(results_file, \"w\") as f:\n",
    "    f.write(\"Benchmarking MiniLM Embeddings with XGBoost (Train/Test Split)\\n\\n\")\n",
    "    f.write(f\"XGBoost: MAE = {mae:.4f}, QWK = {qwk:.4f}\\n\")\n",
    "\n",
    "# Print results\n",
    "print(f\"XGBoost - MAE: {mae:.4f}, QWK: {qwk:.4f}\")\n",
    "print(f\"\\n‚úÖ Benchmarking complete! Results saved in {results_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (3911, 384)\n",
      "Labels shape: (3911, 6)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Loading model: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/datastore/ibrahimh/conda_envs/gpu_39_env1/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Extracting MPNet embeddings...\n",
      "\n",
      "‚úÖ Saved MPNet embeddings to dataset/embeddings_transformer/MPNet_embeddings.npz\n",
      "Total extracted embeddings: 3911\n",
      "Total labels: 3911\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ‚úÖ Load dataset\n",
    "file_path = \"dataset/train.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# ‚úÖ Ensure expected columns exist\n",
    "expected_columns = [\"text_id\", \"full_text\", \"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]\n",
    "if not all(col in data.columns for col in expected_columns):\n",
    "    raise ValueError(f\"Dataset is missing required columns. Expected: {expected_columns}\")\n",
    "\n",
    "# ‚úÖ Drop rows with missing text\n",
    "data = data.dropna(subset=[\"full_text\"]).reset_index(drop=True)\n",
    "\n",
    "# ‚úÖ Extract text data\n",
    "texts = data[\"full_text\"].tolist()\n",
    "labels = data[[\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]].values\n",
    "\n",
    "# ‚úÖ Load MPNet model\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "print(f\"\\nüîç Loading model: {MODEL_NAME}\")\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# ‚úÖ Extract embeddings\n",
    "print(\"\\nüöÄ Extracting MPNet embeddings...\")\n",
    "embeddings = model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "# ‚úÖ Save embeddings and labels properly\n",
    "output_path = \"dataset/embeddings_transformer/MPNet_embeddings.npz\"\n",
    "np.savez(output_path, embeddings=embeddings, labels=labels)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved MPNet embeddings to {output_path}\")\n",
    "print(f\"Total extracted embeddings: {embeddings.shape[0]}\")\n",
    "print(f\"Total labels: {labels.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the file: ['embeddings', 'labels']\n",
      "Embeddings shape: (3911, 768)\n",
      "Labels shape: (3911, 6)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"dataset/embeddings_transformer/MPNet_embeddings.npz\")\n",
    "print(\"Keys in the file:\", list(data.keys()))\n",
    "print(\"Embeddings shape:\", data[\"embeddings\"].shape)\n",
    "print(\"Labels shape:\", data[\"labels\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç **Loading MPNet embeddings**\n",
      "üöÄ Training XGBoost on MPNet embeddings...\n",
      "XGBoost - MAE: 0.3787, QWK: 0.3926\n",
      "\n",
      "‚úÖ Benchmarking complete! Results saved in benchmark_results_mpnet_xgb.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, cohen_kappa_score\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths to MPNet embeddings\n",
    "EMBEDDINGS_FILE = \"dataset/embeddings_transformer/MPNet_embeddings.npz\"\n",
    "\n",
    "# Load MPNet embeddings\n",
    "print(\"\\nüîç **Loading MPNet embeddings**\")\n",
    "data = np.load(EMBEDDINGS_FILE)\n",
    "embeddings, labels = data[\"embeddings\"], data[\"labels\"]\n",
    "\n",
    "# Define Train/Test Split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define valid grade levels\n",
    "VALID_GRADES = torch.tensor([1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])\n",
    "\n",
    "def round_to_valid_grades(predictions):\n",
    "    \"\"\"Round predictions to the nearest valid grade.\"\"\"\n",
    "    return np.array([\n",
    "        float(VALID_GRADES[torch.argmin(torch.abs(VALID_GRADES - pred))])\n",
    "        for pred in predictions\n",
    "    ])\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    \"\"\"Calculate QWK.\"\"\"\n",
    "    grade_to_bin = {float(grade): idx for idx, grade in enumerate(VALID_GRADES)}\n",
    "    y_true_bins = [grade_to_bin[float(val)] for val in y_true]\n",
    "    y_pred_bins = [grade_to_bin[float(val)] for val in y_pred]\n",
    "\n",
    "    if len(set(y_true_bins)) == 1 or len(set(y_pred_bins)) == 1:\n",
    "        return 0.0  # Prevents QWK crash when only one class exists\n",
    "\n",
    "    return cohen_kappa_score(y_true_bins, y_pred_bins, weights=\"quadratic\")\n",
    "\n",
    "# üöÄ Train XGBoost on MPNet embeddings\n",
    "print(\"üöÄ Training XGBoost on MPNet embeddings...\")\n",
    "xgb_model = XGBRegressor(n_estimators=200, max_depth=10, learning_rate=0.1, n_jobs=-1, random_state=42)\n",
    "xgb_model.fit(X_train, y_train.mean(axis=1))\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = xgb_model.predict(X_test)\n",
    "predictions_rounded = round_to_valid_grades(predictions)\n",
    "y_test_rounded = round_to_valid_grades(y_test.mean(axis=1))\n",
    "\n",
    "# Compute MAE and QWK\n",
    "mae = mean_absolute_error(y_test_rounded, predictions_rounded)\n",
    "qwk = quadratic_weighted_kappa(y_test_rounded, predictions_rounded)\n",
    "\n",
    "# Save results\n",
    "results_file = \"benchmark_results_mpnet_xgb.txt\"\n",
    "with open(results_file, \"w\") as f:\n",
    "    f.write(\"Benchmarking MPNet Embeddings with XGBoost (Train/Test Split)\\n\\n\")\n",
    "    f.write(f\"XGBoost: MAE = {mae:.4f}, QWK = {qwk:.4f}\\n\")\n",
    "\n",
    "# Print results\n",
    "print(f\"XGBoost - MAE: {mae:.4f}, QWK: {qwk:.4f}\")\n",
    "print(f\"\\n‚úÖ Benchmarking complete! Results saved in {results_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Loading model: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/datastore/ibrahimh/conda_envs/gpu_39_env1/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Extracting RoBERTa embeddings...\n",
      "\n",
      "‚úÖ Saved RoBERTa embeddings to dataset/embeddings_transformer/RoBERTa_embeddings.npz\n",
      "Total extracted embeddings: 3911\n",
      "Embeddings dimension: 768\n",
      "Total labels: 3911\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ‚úÖ Load dataset\n",
    "file_path = \"dataset/train.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# ‚úÖ Ensure expected columns exist\n",
    "expected_columns = [\"text_id\", \"full_text\", \"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]\n",
    "if not all(col in data.columns for col in expected_columns):\n",
    "    raise ValueError(f\"Dataset is missing required columns. Expected: {expected_columns}\")\n",
    "\n",
    "# ‚úÖ Drop rows with missing text\n",
    "data = data.dropna(subset=[\"full_text\"]).reset_index(drop=True)\n",
    "\n",
    "# ‚úÖ Extract text data\n",
    "texts = data[\"full_text\"].tolist()\n",
    "labels = data[[\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]].values\n",
    "\n",
    "# ‚úÖ Load RoBERTa model & tokenizer\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "print(f\"\\nüîç Loading model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ‚úÖ Function to extract mean-pooled embeddings\n",
    "def extract_embeddings(texts, tokenizer, model):\n",
    "    \"\"\"Get mean-pooled embeddings from RoBERTa.\"\"\"\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            outputs = model(**inputs)\n",
    "            token_embeddings = outputs.last_hidden_state  # Shape: [1, seq_len, hidden_dim]\n",
    "            sentence_embedding = token_embeddings.mean(dim=1).squeeze().numpy()  # Mean pooling\n",
    "            all_embeddings.append(sentence_embedding)\n",
    "\n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "# ‚úÖ Extract embeddings\n",
    "print(\"\\nüöÄ Extracting RoBERTa embeddings...\")\n",
    "embeddings = extract_embeddings(texts, tokenizer, model)\n",
    "\n",
    "# ‚úÖ Save embeddings and labels properly\n",
    "output_path = \"dataset/embeddings_transformer/RoBERTa_embeddings.npz\"\n",
    "np.savez(output_path, embeddings=embeddings, labels=labels)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved RoBERTa embeddings to {output_path}\")\n",
    "print(f\"Total extracted embeddings: {embeddings.shape[0]}\")\n",
    "print(f\"Embeddings dimension: {embeddings.shape[1]}\")  # Should be 768\n",
    "print(f\"Total labels: {labels.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the file: ['embeddings', 'labels']\n",
      "Embeddings shape: (3911, 768)\n",
      "Labels shape: (3911, 6)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"dataset/embeddings_transformer/RoBERTa_embeddings.npz\")\n",
    "print(\"Keys in the file:\", list(data.keys()))\n",
    "print(\"Embeddings shape:\", data[\"embeddings\"].shape)\n",
    "print(\"Labels shape:\", data[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

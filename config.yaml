# GPU Configuration
gpu: 1

# Paths
#train_data_path: './data/stratified_500.csv'
#train_data_path: './data/stratified_100.csv'
#test_data_path: './data/stratified_100.csv'
#test_data_path: './data/few_samples.csv'

#these are stratified splits
#train_data_path: './data/main/train.csv'
#test_data_path: './data/main/test.csv'


#for all data no splits
train_data_path: './data/train_set_kaggle.csv'


#k_folds: 2
k_folds: 5
wandb_on: False
#wandb_on: True
use_subset : False
use_prompt : False
save_best_model: False

# Model parameters
model:
  #path: './models/Llama-3.2-3B-Instruct'
  #path: './models/Llama-3.2-1B-Instruct'
  path: './models/Meta-Llama-3.2-1B'
  version: '3.2'
  num_parameters: '1B'  # Specify the number of parameters (e.g., 1B, 7B, etc.)
  #embeddings_path: './data/embeddings_and_labels_llama32base_maxlen_1214.npz'
  #embeddings_path: './data/doc2vec_embeddings.npy'

checkpoint_dir: './checkpoints/'
prompt_path: './prompts/fine_tune_per_trait.txt'


# Training parameters
#training_strategy: 'last_layer_only'
training_strategy: 'top_n_layers'
#layers_to_unfreeze: 4
layers_to_unfreeze: 6
#training_strategy: 'prompt_tuning'
#training_strategy: 'all_layers'
#training_strategy: 'last_layer_only', 'all_layers', 'prompt_tuning'
epochs: 100
batch_size: 1
max_len: 1214
#max_len: 870 #%95 of the dateset
#max_len: 1024
#max_len: 400

# Optional learning rate based on the training strategy
learning_rate:
  last_layer_only: 5e-5 # 3e-4
  all_layers: 2e-5
  top_n_layers: 5e-5
  prompt_tuning: 2e-3  # Default for promptter tuning

patience: 15

# Optional list of metrics
metrics:
  - accuracy
  - loss

#apply_scaling: false  # toggle scaling output scores to the range [1, 5].
#apply_rounding: false # toggle rounding to the nearest 0.5.
